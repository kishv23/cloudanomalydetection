{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308ee033-2ceb-4442-ba07-432f1c0b419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression -> ERROR: Input X contains NaN.\n",
      "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "Random Forest -> acc: 0.8025, f1 (weighted): 0.8000\n",
      "HistGradientBoosting -> acc: 0.8000, f1 (weighted): 0.7976\n",
      "Extra Trees -> acc: 0.7275, f1 (weighted): 0.7230\n",
      "Gradient Boosting -> ERROR: Input X contains NaN.\n",
      "GradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "AdaBoost -> ERROR: Input X contains NaN.\n",
      "AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "KNN -> ERROR: Input X contains NaN.\n",
      "KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "SVC -> ERROR: Input X contains NaN.\n",
      "SVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "Best model selected: Random Forest {'accuracy': 0.8025, 'f1': 0.7999683494223769}\n",
      "\n",
      "Saved: model.pkl, scaler.pkl, label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# train_models_robust.py\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import inspect\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    AdaBoostClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"small.csv\")   # change path if needed\n",
    "\n",
    "# assume last column is target\n",
    "X = df.iloc[:, :-1].copy()\n",
    "y = df.iloc[:, -1].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Label-encode target\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Prepare OneHotEncoder in a version-safe way\n",
    "# ----------------------------\n",
    "# determine supported parameter name for OneHotEncoder dense output\n",
    "ohe_init_sig = inspect.signature(OneHotEncoder.__init__)\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "if \"sparse_output\" in ohe_init_sig.parameters:\n",
    "    ohe_kwargs[\"sparse_output\"] = False\n",
    "elif \"sparse\" in ohe_init_sig.parameters:\n",
    "    ohe_kwargs[\"sparse\"] = False\n",
    "else:\n",
    "    # fallback: do not pass either (should be rare)\n",
    "    pass\n",
    "\n",
    "ohe = OneHotEncoder(**ohe_kwargs)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Identify categorical and numeric columns\n",
    "# ----------------------------\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Fit a standalone scaler on numeric columns (so we can save scaler.pkl)\n",
    "if len(num_cols) > 0:\n",
    "    numeric_scaler_fitted = StandardScaler().fit(X[num_cols])\n",
    "else:\n",
    "    numeric_scaler_fitted = None\n",
    "\n",
    "# For pipeline we provide a fresh transformer instance (it will be fitted inside pipeline.fit)\n",
    "numeric_scaler_for_pipeline = StandardScaler()\n",
    "\n",
    "# Build ColumnTransformer\n",
    "transformers = []\n",
    "if len(cat_cols) > 0:\n",
    "    transformers.append((\"cat\", ohe, cat_cols))\n",
    "if len(num_cols) > 0:\n",
    "    transformers.append((\"num\", numeric_scaler_for_pipeline, num_cols))\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocess = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Define models to train\n",
    "# ----------------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_jobs=-1),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Train & evaluate\n",
    "# ----------------------------\n",
    "# we'll use a single train/test split for quick comparisons\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "for name, estimator in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", estimator)\n",
    "    ])\n",
    "    try:\n",
    "        pipe.fit(X_train_full, y_train_full)\n",
    "        preds = pipe.predict(X_test_full)\n",
    "        acc = accuracy_score(y_test_full, preds)\n",
    "        f1 = f1_score(y_test_full, preds, average=\"weighted\")\n",
    "        results[name] = {\"accuracy\": acc, \"f1\": f1}\n",
    "        print(f\"{name} -> acc: {acc:.4f}, f1 (weighted): {f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        # If a model can't train for some dataset shapes, record the error and continue\n",
    "        results[name] = {\"error\": str(e)}\n",
    "        print(f\"{name} -> ERROR: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Choose best model (by weighted F1)\n",
    "# ----------------------------\n",
    "# filter out models that errored\n",
    "valid_results = {k: v for k, v in results.items() if \"f1\" in v}\n",
    "if not valid_results:\n",
    "    raise RuntimeError(\"No model trained successfully. See printed errors above.\")\n",
    "\n",
    "best_name = max(valid_results, key=lambda k: valid_results[k][\"f1\"])\n",
    "print(\"\\nBest model selected:\", best_name, valid_results[best_name])\n",
    "\n",
    "best_estimator = models[best_name]\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Retrain best model on full dataset\n",
    "# ----------------------------\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", best_estimator)\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X, y_encoded)  # fit on entire data\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Save artifacts\n",
    "# ----------------------------\n",
    "joblib.dump(final_pipeline, \"model.pkl\")\n",
    "# if numeric_scaler_fitted is None, save None to indicate not applicable\n",
    "joblib.dump(numeric_scaler_fitted, \"scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "print(\"\\nSaved: model.pkl, scaler.pkl, label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9e0f0a-f5d8-42c5-8973-75e31d854ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1998, 13)\n",
      "Dropping identifier/time columns: ['vm_id', 'timestamp']\n",
      "\n",
      "Missing per column:\n",
      " cpu_usage                    260\n",
      "memory_usage                 190\n",
      "network_traffic              208\n",
      "power_consumption            209\n",
      "num_executed_instructions    200\n",
      "execution_time               228\n",
      "energy_efficiency            142\n",
      "task_type                    179\n",
      "task_priority                203\n",
      "task_status                  190\n",
      "dtype: int64\n",
      "\n",
      "Target distribution:\n",
      " Anomaly status\n",
      "1    999\n",
      "0    999\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical cols: ['task_type', 'task_priority', 'task_status']\n",
      "Numeric cols: ['cpu_usage', 'memory_usage', 'network_traffic', 'power_consumption', 'num_executed_instructions', 'execution_time', 'energy_efficiency']\n",
      "\n",
      "Evaluating LogisticRegression ...\n",
      "  CV F1 (weighted): 0.7093 +/- 0.0196\n",
      "\n",
      "Evaluating RandomForest ...\n",
      "  CV F1 (weighted): 0.7717 +/- 0.0147\n",
      "\n",
      "Evaluating HistGradientBoosting ...\n",
      "  CV F1 (weighted): 0.7801 +/- 0.0089\n",
      "\n",
      "Evaluating ExtraTrees ...\n",
      "  CV F1 (weighted): 0.6959 +/- 0.0261\n",
      "\n",
      "Evaluating GradientBoosting ...\n",
      "  CV F1 (weighted): 0.7842 +/- 0.0135\n",
      "\n",
      "Evaluating AdaBoost ...\n",
      "  CV F1 (weighted): 0.7545 +/- 0.0147\n",
      "\n",
      "Evaluating KNN ...\n",
      "  CV F1 (weighted): 0.6568 +/- 0.0223\n",
      "\n",
      "Evaluating SVC ...\n",
      "  CV F1 (weighted): 0.7121 +/- 0.0183\n",
      "\n",
      "Best model selected: GradientBoosting {'cv_f1_mean': 0.7842197300894369, 'cv_f1_std': 0.013526215972283364}\n",
      "\n",
      "Fitting final pipeline on full dataset ...\n",
      "\n",
      "Final model test accuracy: 0.8375\n",
      "Final model test F1 (weighted): 0.8368627450980393\n",
      "Could not evaluate final model on X_test: object of type 'numpy.int64' has no len()\n",
      "\n",
      "Saved artifacts: model.pkl, scaler.pkl, label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# retrain_fixed.py\n",
    "\"\"\"\n",
    "Robust training script that:\n",
    " - Drops identifier/time columns (vm_id, timestamp) by default\n",
    " - Imputes missing values for numeric/categorical features\n",
    " - Uses OneHotEncoder in a version-safe way\n",
    " - Avoids multiprocessing (no n_jobs=-1 or parallel CV)\n",
    " - Falls back to single-fit evaluation if CV fails\n",
    " - Saves: model.pkl, scaler.pkl, label_encoder.pkl\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import inspect\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"small.csv\")   # adjust path if needed\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Drop identifier/timecolumns (common in VM logs)\n",
    "# ----------------------------\n",
    "drop_cols = []\n",
    "for c in [\"vm_id\", \"timestamp\"]:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "if drop_cols:\n",
    "    print(\"Dropping identifier/time columns:\", drop_cols)\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Prepare X, y\n",
    "# ----------------------------\n",
    "# assume last column is the target (as before)\n",
    "X = df.iloc[:, :-1].copy()\n",
    "y = df.iloc[:, -1].copy()\n",
    "\n",
    "print(\"\\nMissing per column:\\n\", X.isna().sum())\n",
    "print(\"\\nTarget distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Label encode target\n",
    "# ----------------------------\n",
    "label_enc = LabelEncoder()\n",
    "y_enc = label_enc.fit_transform(y)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Identify categorical and numeric columns\n",
    "# ----------------------------\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical cols:\", cat_cols)\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Safe OneHotEncoder for different sklearn versions\n",
    "# ----------------------------\n",
    "ohe_init_sig = inspect.signature(OneHotEncoder.__init__)\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "if \"sparse_output\" in ohe_init_sig.parameters:\n",
    "    ohe_kwargs[\"sparse_output\"] = False\n",
    "elif \"sparse\" in ohe_init_sig.parameters:\n",
    "    ohe_kwargs[\"sparse\"] = False\n",
    "\n",
    "ohe = OneHotEncoder(**ohe_kwargs)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Imputers & scalers\n",
    "# ----------------------------\n",
    "num_imputer = SimpleImputer(strategy=\"median\")     # robust to outliers\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\", fill_value=\"__MISSING__\")\n",
    "scaler_for_pipeline = StandardScaler()\n",
    "\n",
    "# Fit a standalone scaler for numeric columns (to save scaler.pkl)\n",
    "if len(num_cols) > 0:\n",
    "    # fit on full numeric data after imputing median to avoid NaNs\n",
    "    num_data = X[num_cols].copy()\n",
    "    num_data_imputed = num_imputer.fit_transform(num_data)\n",
    "    fitted_scaler = StandardScaler().fit(num_data_imputed)\n",
    "else:\n",
    "    fitted_scaler = None\n",
    "\n",
    "# For pipeline preprocessor create fresh instances (they will be fitted inside the pipeline)\n",
    "num_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]) if num_cols else \"drop\"\n",
    "cat_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\", fill_value=\"__MISSING__\")), (\"ohe\", ohe)]) if cat_cols else \"drop\"\n",
    "\n",
    "transformers = []\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", cat_pipeline, cat_cols))\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_pipeline, num_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Models (no heavy parallelism)\n",
    "# ----------------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    # avoid n_jobs=-1 to prevent multiprocessing issues in constrained envs\n",
    "    \"RandomForest\": RandomForestClassifier(n_jobs=1, random_state=42),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_jobs=1, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Train-test split\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.20, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10. Evaluate with CV (safe: n_jobs=1) with fallback handling\n",
    "# ----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "for name, estimator in models.items():\n",
    "    pipe = Pipeline([(\"preprocessor\", preprocessor), (\"model\", estimator)])\n",
    "    print(f\"\\nEvaluating {name} ...\")\n",
    "    try:\n",
    "        # Use cross_val_score with n_jobs=1 to avoid multiprocessing issues\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"f1_weighted\", n_jobs=1)\n",
    "        results[name] = {\"cv_f1_mean\": float(scores.mean()), \"cv_f1_std\": float(scores.std())}\n",
    "        print(f\"  CV F1 (weighted): {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "    except Exception as e:\n",
    "        # If CV fails, attempt a simple fit on train and evaluate on test (safe single-run)\n",
    "        print(f\"  CV failed for {name}: {e}\")\n",
    "        try:\n",
    "            pipe.fit(X_train, y_train)\n",
    "            preds = pipe.predict(X_test)\n",
    "            acc = accuracy_score(y_test, preds)\n",
    "            f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "            results[name] = {\"cv_f1_mean\": float(f1), \"cv_f1_std\": 0.0, \"note\": \"fallback_single_eval\"}\n",
    "            print(f\"  Fallback evaluation -> acc: {acc:.4f}, f1 (weighted): {f1:.4f}\")\n",
    "        except Exception as e2:\n",
    "            results[name] = {\"error\": str(e2)}\n",
    "            print(f\"  Fallback also failed for {name}: {e2}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11. Select best model\n",
    "# ----------------------------\n",
    "valid = {k: v for k, v in results.items() if \"cv_f1_mean\" in v}\n",
    "if not valid:\n",
    "    raise RuntimeError(\"No model evaluated successfully. See printed errors above.\")\n",
    "\n",
    "best_name = max(valid, key=lambda k: valid[k][\"cv_f1_mean\"])\n",
    "print(\"\\nBest model selected:\", best_name, valid[best_name])\n",
    "best_estimator = models[best_name]\n",
    "\n",
    "# ----------------------------\n",
    "# 12. Retrain best model on FULL DATA\n",
    "# ----------------------------\n",
    "final_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", best_estimator)])\n",
    "print(\"\\nFitting final pipeline on full dataset ...\")\n",
    "final_pipeline.fit(X, y_enc)\n",
    "\n",
    "# ----------------------------\n",
    "# 13. Evaluate final model on holdout (X_test)\n",
    "# ----------------------------\n",
    "try:\n",
    "    preds_test = final_pipeline.predict(X_test)\n",
    "    print(\"\\nFinal model test accuracy:\", accuracy_score(y_test, preds_test))\n",
    "    print(\"Final model test F1 (weighted):\", f1_score(y_test, preds_test, average=\"weighted\"))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, preds_test, target_names=label_enc.classes_))\n",
    "except Exception as e:\n",
    "    print(\"Could not evaluate final model on X_test:\", e)\n",
    "\n",
    "# ----------------------------\n",
    "# 14. Save artifacts\n",
    "# ----------------------------\n",
    "joblib.dump(final_pipeline, \"model.pkl\")\n",
    "joblib.dump(fitted_scaler, \"scaler.pkl\")\n",
    "joblib.dump(label_enc, \"label_encoder.pkl\")\n",
    "print(\"\\nSaved artifacts: model.pkl, scaler.pkl, label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fad8544-fa11-4fce-866f-509325b61726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating confusion matrices for all models...\n",
      "\n",
      "\n",
      "LogisticRegression Confusion Matrix:\n",
      "[[115  85]\n",
      " [ 37 163]]\n",
      "\n",
      "Saved: cm_LogisticRegression.png\n",
      "\n",
      "RandomForest Confusion Matrix:\n",
      "[[149  51]\n",
      " [ 29 171]]\n",
      "\n",
      "Saved: cm_RandomForest.png\n",
      "\n",
      "HistGradientBoosting Confusion Matrix:\n",
      "[[137  63]\n",
      " [ 27 173]]\n",
      "\n",
      "Saved: cm_HistGradientBoosting.png\n",
      "\n",
      "ExtraTrees Confusion Matrix:\n",
      "[[127  73]\n",
      " [ 49 151]]\n",
      "\n",
      "Saved: cm_ExtraTrees.png\n",
      "\n",
      "GradientBoosting Confusion Matrix:\n",
      "[[145  55]\n",
      " [ 29 171]]\n",
      "\n",
      "Saved: cm_GradientBoosting.png\n",
      "\n",
      "AdaBoost Confusion Matrix:\n",
      "[[155  45]\n",
      " [ 44 156]]\n",
      "\n",
      "Saved: cm_AdaBoost.png\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[109  91]\n",
      " [ 65 135]]\n",
      "\n",
      "Saved: cm_KNN.png\n",
      "\n",
      "SVC Confusion Matrix:\n",
      "[[118  82]\n",
      " [ 37 163]]\n",
      "\n",
      "Saved: cm_SVC.png\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 13B. Confusion Matrix for each model\n",
    "# ----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "print(\"\\nGenerating confusion matrices for all models...\\n\")\n",
    "\n",
    "for name, estimator in models.items():\n",
    "    try:\n",
    "        pipe = Pipeline([(\"preprocessor\", preprocessor), (\"model\", estimator)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        preds = pipe.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        print(f\"\\n{name} Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_enc.classes_)\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "        plt.title(f\"Confusion Matrix - {name}\")\n",
    "\n",
    "        # Save confusion matrix PNG\n",
    "        fig.savefig(f\"cm_{name}.png\", dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved: cm_{name}.png\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not generate confusion matrix for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6026ee-14f8-4c1d-8f14-0130325a27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4\n",
    "!pip install pandas==1.5.3\n",
    "!pip install scikit-learn==1.3.2\n",
    "!pip install joblib==1.3.2\n",
    "!pip install matplotlib==3.8.2\n",
    "!pip install seaborn==0.13.1\n",
    "!pip install plotly==5.18.0\n",
    "!pip install shap==0.42.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5c817-f615-476b-b242-e5311873b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain_fixed.py\n",
    "\"\"\"\n",
    "Robust training script that:\n",
    " - Drops identifier/time columns (vm_id, timestamp) by default\n",
    " - Imputes missing values for numeric/categorical features\n",
    " - Uses OneHotEncoder in a version-safe way\n",
    " - Avoids multiprocessing (no n_jobs=-1 or parallel CV)\n",
    " - Falls back to single-fit evaluation if CV fails\n",
    " - Saves: model.pkl, scaler.pkl, label_encoder.pkl\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import inspect\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load data\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"small.csv\")   # adjust path if needed\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Drop identifier/timecolumns (common in VM logs)\n",
    "# ----------------------------\n",
    "drop_cols = []\n",
    "for c in [\"vm_id\", \"timestamp\"]:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "if drop_cols:\n",
    "    print(\"Dropping identifier/time columns:\", drop_cols)\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Prepare X, y\n",
    "# ----------------------------\n",
    "# assume last column is the target (as before)\n",
    "X = df.iloc[:, :-1].copy()\n",
    "y = df.iloc[:, -1].copy()\n",
    "\n",
    "print(\"\\nMissing per column:\\n\", X.isna().sum())\n",
    "print(\"\\nTarget distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Label encode target\n",
    "# ----------------------------\n",
    "label_enc = LabelEncoder()\n",
    "y_enc = label_enc.fit_transform(y)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Identify categorical and numeric columns\n",
    "# ----------------------------\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical cols:\", cat_cols)\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Safe OneHotEncoder for different sklearn versions\n",
    "# ----------------------------\n",
    "ohe_init_sig = inspect.signature(OneHotEncoder.__init__)\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "if \"sparse_output\" in ohe_init_sig.parameters:\n",
    "    ohe_kwargs[\"sparse_output\"] = False\n",
    "elif \"sparse\" in ohe_init_sig.parameters:\n",
    "    ohe_kwargs[\"sparse\"] = False\n",
    "\n",
    "ohe = OneHotEncoder(**ohe_kwargs)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Imputers & scalers\n",
    "# ----------------------------\n",
    "num_imputer = SimpleImputer(strategy=\"median\")     # robust to outliers\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\", fill_value=\"__MISSING__\")\n",
    "scaler_for_pipeline = StandardScaler()\n",
    "\n",
    "# Fit a standalone scaler for numeric columns (to save scaler.pkl)\n",
    "if len(num_cols) > 0:\n",
    "    # fit on full numeric data after imputing median to avoid NaNs\n",
    "    num_data = X[num_cols].copy()\n",
    "    num_data_imputed = num_imputer.fit_transform(num_data)\n",
    "    fitted_scaler = StandardScaler().fit(num_data_imputed)\n",
    "else:\n",
    "    fitted_scaler = None\n",
    "\n",
    "# For pipeline preprocessor create fresh instances (they will be fitted inside the pipeline)\n",
    "num_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]) if num_cols else \"drop\"\n",
    "cat_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\", fill_value=\"__MISSING__\")), (\"ohe\", ohe)]) if cat_cols else \"drop\"\n",
    "\n",
    "transformers = []\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", cat_pipeline, cat_cols))\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_pipeline, num_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Models (no heavy parallelism)\n",
    "# ----------------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    # avoid n_jobs=-1 to prevent multiprocessing issues in constrained envs\n",
    "    \"RandomForest\": RandomForestClassifier(n_jobs=1, random_state=42),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_jobs=1, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Train-test split\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.20, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10. Evaluate with CV (safe: n_jobs=1) with fallback handling\n",
    "# ----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "for name, estimator in models.items():\n",
    "    pipe = Pipeline([(\"preprocessor\", preprocessor), (\"model\", estimator)])\n",
    "    print(f\"\\nEvaluating {name} ...\")\n",
    "    try:\n",
    "        # Use cross_val_score with n_jobs=1 to avoid multiprocessing issues\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"f1_weighted\", n_jobs=1)\n",
    "        results[name] = {\"cv_f1_mean\": float(scores.mean()), \"cv_f1_std\": float(scores.std())}\n",
    "        print(f\"  CV F1 (weighted): {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "    except Exception as e:\n",
    "        # If CV fails, attempt a simple fit on train and evaluate on test (safe single-run)\n",
    "        print(f\"  CV failed for {name}: {e}\")\n",
    "        try:\n",
    "            pipe.fit(X_train, y_train)\n",
    "            preds = pipe.predict(X_test)\n",
    "            acc = accuracy_score(y_test, preds)\n",
    "            f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "            results[name] = {\"cv_f1_mean\": float(f1), \"cv_f1_std\": 0.0, \"note\": \"fallback_single_eval\"}\n",
    "            print(f\"  Fallback evaluation -> acc: {acc:.4f}, f1 (weighted): {f1:.4f}\")\n",
    "        except Exception as e2:\n",
    "            results[name] = {\"error\": str(e2)}\n",
    "            print(f\"  Fallback also failed for {name}: {e2}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 11. Select best model\n",
    "# ----------------------------\n",
    "valid = {k: v for k, v in results.items() if \"cv_f1_mean\" in v}\n",
    "if not valid:\n",
    "    raise RuntimeError(\"No model evaluated successfully. See printed errors above.\")\n",
    "\n",
    "best_name = max(valid, key=lambda k: valid[k][\"cv_f1_mean\"])\n",
    "print(\"\\nBest model selected:\", best_name, valid[best_name])\n",
    "best_estimator = models[best_name]\n",
    "\n",
    "# ----------------------------\n",
    "# 12. Retrain best model on FULL DATA\n",
    "# ----------------------------\n",
    "final_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", best_estimator)])\n",
    "print(\"\\nFitting final pipeline on full dataset ...\")\n",
    "final_pipeline.fit(X, y_enc)\n",
    "\n",
    "# ----------------------------\n",
    "# 13. Evaluate final model on holdout (X_test)\n",
    "# ----------------------------\n",
    "try:\n",
    "    preds_test = final_pipeline.predict(X_test)\n",
    "    print(\"\\nFinal model test accuracy:\", accuracy_score(y_test, preds_test))\n",
    "    print(\"Final model test F1 (weighted):\", f1_score(y_test, preds_test, average=\"weighted\"))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, preds_test, target_names=label_enc.classes_))\n",
    "except Exception as e:\n",
    "    print(\"Could not evaluate final model on X_test:\", e)\n",
    "\n",
    "# ----------------------------\n",
    "# 14. Save artifacts\n",
    "# ----------------------------\n",
    "joblib.dump(final_pipeline, \"model.pkl\")\n",
    "joblib.dump(fitted_scaler, \"scaler.pkl\")\n",
    "joblib.dump(label_enc, \"label_encoder.pkl\")\n",
    "print(\"\\nSaved artifacts: model.pkl, scaler.pkl, label_encoder.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
